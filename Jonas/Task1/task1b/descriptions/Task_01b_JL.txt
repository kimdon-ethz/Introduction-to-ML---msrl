First, each data row is expanded, by applying the features on x1-x5 and writing it in the correct order. On the new, expanded Data Matrix, linear regression is performed. We choose to analyze the impact of Regularization (Reg) on the cross validated RMSE. Ridge, Lasso and a combined Ridge+Lasso ("Elastic") was used. However, initial results showed that pure Ridge Reg performed better - therefore final results were obtained with Ridge. 

The Reg Parameter and other hyperparameters (Optimizer,folds,..) were tuned by minimizing the estimated risk, that was obtained via Cross-Validation (CV). One could use Leave-One-Out CV (700 Folds for the 700 Datapoints)to minimize the bias of the model when generalizing to the test set. However, this leads to high computation times, as well as higher variance. Therefore, I used 175 folds in the CV as a trade-off.

To find the optimal Reg Parameter, we looped over a logspace of potential candidates and then over a finer linspace to get a better estimate. For each Reg Parameter, the RMS is averaged over all test folds. The Code loops as well over a list of optimizers that can be used in the Ridge function. The output is the best Reg parameter for each optimizer. The different amount of folds, tolerance and max iterations in the optimizer were manually changed in several trials. This led to the final hyperparameters, that minimize the estimated risk. With these, the linear regression was performed again on the whole dataset to obtain the weights.

